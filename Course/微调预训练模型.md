# 第三章 微调预训练模型





## 介绍

​		在第2章中，我们探讨了如何使用分词器和预训练模型进行预测。但是，如果您想为自己的数据集微调预训练模型，该怎么办？这就是本章的主题！

+ 如何从 Hub 准备大型数据集
+ 如何使用高级 `Trainer`API 微调模型
+ 如何使用自定义训练循环
+ 如何利用  Accelerate 库在任何分布式设置上轻松运行该自定义训练循环



## 处理数据

​		继续上一章的例子，下面是我们如何在PyTorch中的一个批处理上训练一个序列分类器：

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
print(batch)

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

​		当然，仅仅用两句话训练模型不会产生非常好的结果。为了获得更好的结果，您需要准备一个更大的数据集。

​		在本节中，我们将使用MRPC（Microsoft Research Paraphrase Corpus）数据集作为示例，该数据集由William B. Dolan和Chris Brockett在一篇论文中介绍。该数据集由5，801对句子组成，带有一个标签，指示它们是否是释义（即，如果两个句子的含义相同）。我们在本章中选择了它，因为它是一个小数据集，所以很容易对其进行训练。



### 从中心加载数据集

​		Hub不仅包含模型;它还具有许多不同语言的多个数据集。您可以在此处浏览数据集，我们建议您在完成本节后尝试加载和处理新数据集（请参阅此处的一般文档）。但现在，让我们专注于MRPC数据集！这是组成 GLUE 基准测试的 10 个数据集之一，GLUE 基准测试是一个学术基准，用于衡量 ML 模型在 10 个不同文本分类任务中的性能。

​		数据集库提供了一个非常简单的命令，用于在 Hub 上下载和缓存数据集。我们可以像这样下载MRPC数据集：

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

运行结果：

```python
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

​		如您所见，我们得到一个包含训练集、验证集和测试集的`DatasetDict`对象。其中每个都包含几列（sentence1，sentence2 ，label 和 idx） 和可变行数，这些行是每个集中的元素数（因此，训练集中有 3，668 对句子，验证集中有 408 对，测试集中有 1，725 对）。

​		此命令下载并缓存数据集，默认情况下在 *~/.cache/huggingface/datasets 中*。回想一下第 2 章中，您可以通过设置`HF_HOME`环境变量来自定义缓存文件夹。

​		我们可以通过索引来访问`raw_datasets`对象中的每对句子，就像使用字典一样：

```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

运行结果：

```python
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', 
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .', 
 'label': 1, 
 'idx': 0}
```

​		我们可以看到标签已经是整数，因此我们不必在那里进行任何预处理。要知道哪个整数对应于哪个标签，我们可以检查我们的`raw_train_dataset`.这将告诉我们每列的类型：

```python
raw_train_dataset.features
```

运行结果：

```python
{'sentence1': Value(dtype='string', id=None), 
 'sentence2': Value(dtype='string', id=None), 
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], id=None), 
 'idx': Value(dtype='int32', id=None)}
```

​		在后台，label 类型为 ClassLabel  ，整数到标签名称的映射存储在 *names* 文件夹中。0 对应于 `not_equivalent` ，并且1对应于equivalent 。



### 预处理数据集

​		要预处理数据集，我们需要将文本转换为模型可以理解的数字。正如您在上一章中看到的，这是使用分词器完成的。我们可以向分词器提供一个句子或一系列句子，这样我们就可以直接标记每对的所有第一个句子和所有第二个句子，如下所示：

```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

​		但是，我们不能只是将两个序列传递给模型，然后预测这两个句子是否是释义。我们需要将这两个序列作为一对处理，并应用适当的预处理。幸运的是，分词器还可以采用一对序列，并按照我们的BERT模型期望的方式进行准备：

```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

运行结果：

```python
{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

​		我们在第 2 章中讨论了 input_ids 和 `attention_mask`  键，但是我们推迟了 `token_type_ids`讨论 。在此示例中，这是告诉模型输入的哪一部分是第一个句子，哪个是第二个句子。

​		如果我们将里面的 `input_ids` ID解码回单词：

```python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

​		我们将得到：

```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

​		因此，我们看到，当有两个句子时，模型期望输入是[CLS] sentence1 [SEP] sentence2 [SEP]的形式。将其与token_type_ids对齐可以得到:

```
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

​		可以看到，与[CLS] sentence1 [SEP]相对应的输入部分都有一个标记类型ID为0，而与sentence2 [SEP]相对应的其他部分都有一个标记类型ID为1。

​		注意，如果选择不同的检查点，则在标记化的输入中不一定有token_type_ids(例如，如果使用DistilBERT模型，则不会返回它们)。只有当模型知道如何处理它们时，它们才会被返回，因为它已经在训练前看到了它们。

​		在这里，BERT使用标记类型id进行了预训练，在我们在第一章中讨论的屏蔽语言建模目标之上，它还有一个额外的目标，称为下一句预测。这个任务的目标是模拟句子之间的关系。

​		对于下一个句子的预测，该模型被提供成对的句子(带有随机掩码标记)，并被要求预测第二个句子是否跟在第一个句子之后。为了使任务不平凡，一半的时间句子在提取它们的原始文档中相互跟随，另一半时间两个句子来自两个不同的文档。

​		通常，您不需要担心在您的标记化输入中是否有token_type_ids:只要您为标记赋予器和模型使用相同的检查点，一切都会很好，因为标记赋予器知道向其模型提供什么。

​		现在我们已经看到了标记赋予器如何处理一对句子，我们可以使用它来标记我们的整个数据集:就像在前一章中一样，我们可以向标记赋予器提供一对句子的列表，先给出第一个句子的列表，然后给出第二个句子的列表。这也与我们在第2章中看到的填充和截断选项兼容。因此，预处理训练数据集的一种方法是

```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

​		这工作得很好，但是它的缺点是返回一个字典(使用我们的键、input_ids、attention_mask和token_type_ids，以及列表中的列表值)。如果在标记化期间有足够的RAM来存储整个数据集，那么它也只能工作(而来自珞datasets库的数据集是存储在磁盘上的Apache Arrow文件，因此只将请求的示例加载在内存中)。

​		为了将数据保持为数据集，我们将使用dataset .map()方法。如果我们需要做更多的预处理，而不仅仅是标记化，那么这也为我们提供了一些额外的灵活性。map()方法的工作方式是对数据集的每个元素应用一个函数，因此让我们定义一个对输入进行标记的函数

```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

​		此函数接受一个字典(如数据集的条目)，并返回一个新字典，其中包含键input_ids、attention_mask和token_type_ids。注意，如果示例字典包含几个示例(每个键是一个句子列表)，它也可以工作，因为标记赋予器工作于句子对列表，如前所述。这将允许我们在调用map()时使用选项batch =True，这将大大加快标记化的速度。标记赋予器由一个来自珞tokenzers库的用Rust编写的标记赋予器支持。这个标记赋予器可以非常快，但前提是我们一次给它很多输入。

​		注意，我们现在在标记化函数中没有使用填充参数。这是因为填充所有样本到最大长度的效率不高:当我们重新构建批处理时，最好填充样本，因为这样我们只需要填充该批中的最大长度，而不是整个数据集的最大长度。当输入长度可变时，这可以节省大量的时间和处理能力.

​		下面是我们如何一次性在所有数据集上应用标记化函数。我们在对map的调用中使用了batched=True，因此该函数将同时应用于数据集中的多个元素，而不是分别应用于每个元素。这允许更快的预处理。

```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

​		Datasets库应用此处理的方法是向数据集添加新字段，每个字段对应预处理函数返回的字典中的每个键

运行结果：

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

​		通过传递num_proc参数，甚至可以在对map()应用预处理函数时使用multiprocessing。我们在这里没有这样做，因为Tokenizers库已经使用多个线程来更快地对我们的示例进行标记，但是如果您没有使用这个库支持的快速标记分析器，这可能会加快预处理的速度。

​		我们的tokenize_function返回一个字典，其中包含键input_ids、attention_mask和token_type_ids，因此这三个字段被添加到数据集的所有分割部分。注意，如果预处理函数为应用map()的数据集中的现有键返回新值，则可能已经更改了现有字段。

​		最后我们需要做的是，当我们将元素批处理在一起时，将所有示例填充到最长元素的长度，我们称之为动态填充。

### 动态填充

​		负责将样本放在批处理中的函数称为collate函数。它是一个你在构建DataLoader时可以传递的参数，默认值是一个函数，它只会将样本转换为PyTorch张量并连接它们(如果元素是列表、元组或字典，则递归地)。这在我们的例子中是不可能的，因为我们的输入不可能都是相同的大小。我们已经故意推迟填充，只在每批需要时使用它，并避免有过多填充的过长的输入。这将大大加快训练速度，但请注意，如果您是在TPU上训练，这可能会导致TPU更喜欢固定形状的问题，即使这需要额外的填充。

​		为了在实践中做到这一点，我们必须定义一个collate函数，它将对我们想要一起批处理的数据集的项应用正确的填充量。幸运的是，Transformer库通过DataCollatorWithPadding为我们提供了这样一个函数。当您实例化它时，它需要一个标记赋予器(以知道要使用哪个填充标记，以及模型期望填充是在输入的左边还是右边)，并将执行您需要的所有操作:

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

​		为了测试这个新玩具，让我们从我们的训练集中抓取一些我们想要批量处理的样本。在这里，我们删除列idx, sentence1和sentence2，因为它们不需要，并且包含字符串(我们不能用字符串创建张量)，并查看批处理中每个条目的长度:

```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

运行结果：

```python
[50, 59, 47, 67, 59, 50, 62, 32]
```

​		毫无疑问，我们得到的样本长度从32到67不等。动态填充是指该批样品全部填充到67的长度，这是该批内的最大长度。如果没有动态填充，所有的样本都必须填充到整个数据集的最大长度，或者模型可以接受的最大长度。让我们再次检查data_collator是否正确地动态填充了批处理:

```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

运行结果：

```python
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

​		看上去不错!现在，我们已经从原始文本到模型可以处理的批处理，我们准备对其进行微调.



## 使用训练器API或Keras微调模型



### 使用训练器API微调一个模型

​		Transformer提供了一个Trainer类来帮助您微调它在数据集中提供的任何预训练模型。在上一节中完成所有数据预处理工作后，就只剩下几个步骤来定义Trainer了。最难的部分可能是让环境准备好运行Trainer.train()，因为它在CPU上的运行速度很慢。如果您没有设置GPU，您可以在谷歌Colab上获得免费的GPU或tpu。

​		下面的代码示例假设您已经执行了上一节中的示例。这里有一个简短的总结，概括你需要什么：

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```



**训练**

​		在定义Trainer之前，第一步是定义一个TrainingArguments类，它将包含Trainer用于训练和评估的所有超参数。您必须提供的惟一参数是保存训练过的模型的目录，以及沿途的检查点。对于其他所有操作，您可以保留默认值，对于基本的微调来说，默认值应该可以很好地工作。

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

​		第二步是定义我们的模型。和前一章一样，我们将使用带有两个标签的automodelforsequencecclassification类:

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

​		您将注意到，与第2章不同的是，在实例化这个预先训练的模型后，您会得到一个警告。这是因为BERT没有对句子对进行分类的预训练，所以将预训练模型的头部丢弃，添加一个适合于序列分类的新头部。这些警告表明，一些权重没有被使用(与丢失的预训练头对应的权重)，而其他一些权重被随机初始化(与新头对应的权重)。它鼓励你训练模型，这正是我们现在要做的。

​		一旦建立了模型，就可以通过向它传递迄今为止构造的所有对象(模型、training_args、训练和验证数据集、data_collator和tokenizer)来定义一个Trainer:

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

​		请注意，当您像我们这里所做的那样传递标记赋予器时，培训器使用的默认data_collator将是前面定义的DataCollatorWithPadding，因此您可以在此调用中跳过data_collator=data_collator这一行。在第2部分向你们展示这部分的处理过程仍然很重要!

​		要对数据集中的模型进行微调，只需调用Trainer的train()方法:

```python
trainer.train()
```

​		这将开始微调(在GPU上需要几分钟)，并每500步报告一次训练损失。但是，它不会告诉您模型的性能有多好(或多差)。这是因为:

1、我们没有告诉训练器在训练期间通过将evaluation_strategy设置为“steps”(评估每个eval_steps)或“epoch”(在每个epoch结束时评估)来进行评估。

2、我们没有向培训器提供compute_metrics()函数来在上述评估期间计算指标(否则评估只会打印损失，这不是一个非常直观的数字)。



**评估**

​		让我们看看如何构建一个有用的compute_metrics()函数，并在下次训练时使用它。该函数必须接受EvalPrediction对象(这是一个有预测字段和label_ids字段的命名元组)，并返回一个将字符串映射到浮点数的字典(字符串是返回的度量的名称，浮点数是它们的值)。要从模型中获得一些预测，可以使用Trainer.predict()命令：

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

运行结果：

```python
(408, 2) (408,)
```

​		predict()方法的输出是另一个命名元组，它有三个字段:prediction、label_ids和metrics。metrics字段将只包含所传递数据集上的损失，以及一些时间指标(总共和平均花费多长时间进行预测)。一旦完成了compute_metrics()函数并将其传递给培训器，该字段还将包含compute_metrics()返回的指标。

​		正如您所看到的，预测是一个形状为408 x 2的二维数组(408是我们使用的数据集中的元素数量)。这些是我们传递给predict()的数据集中每个元素的日志(正如您在前一章看到的，所有Transformer模型都返回日志)。为了将它们转换为我们可以与标签进行比较的预测，我们需要取第二个轴上值最大的索引:

```python
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

​		我们现在可以将这些猎物与标签进行比较。要构建compute_metric()函数，我们将依赖于Datasets库中的指标。我们可以像加载数据集一样轻松地加载与MRPC数据集相关的指标，这一次使用的是load_metric()函数。返回的对象有一个compute()方法，我们可以用它来进行度量计算:

```python
from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

运行结果：

```python
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

​		您得到的确切结果可能会有所不同，因为模型头部的随机初始化可能会改变它实现的度量。在这里，我们可以看到我们的模型在验证集上的准确率为85.78%，F1得分为89.97。这是用于评估GLUE基准的MRPC数据集上的结果的两个指标。BERT论文中的表格报告基本模型的F1得分为88.9。这是未装箱模型，而我们目前使用的是装箱模型，这解释了更好的结果。

​		将所有内容打包在一起，得到compute_metrics()函数:

```python
def compute_metrics(eval_preds):
    metric = load_metric("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

​		为了查看它在每个纪元结束时用于报告指标的实际效果，下面是我们如何用这个compute_metrics()函数定义一个新的Trainer:

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

​		请注意，我们创建了一个新的TrainingArguments，其evaluation_strategy设置为“epoch”，并创建了一个新的模型，否则，我们将只是继续我们已经训练过的模型的训练。为了启动新的训练，我们执行:

```python
trainer.train()
```

​		这一次，它将在训练损失的基础上，在每个阶段结束时报告验证损失和度量。同样，由于模型的头部初始化是随机的，所以你得到的精确精度/F1分数可能与我们发现的有点不同，但它应该是在同一个范围内。

​		Trainer将在多个gpu或tpu上开箱即用，并提供许多选项，如混合精度训练(在训练参数中使用fp16 = True)。我们将在第10章讨论它所支持的一切。

​		这就结束了使用Trainer API进行微调的介绍。第7章将给出一个用于大多数常见NLP任务的例子，但现在让我们看看如何在纯PyTorch中做同样的事情。





## 完整的训练



​		现在，我们将看到如何在不使用Trainer类的情况下获得与上一节相同的结果。同样，我们假设您已经完成了第2部分中的数据处理。这里有一个简短的总结，涵盖了你需要的一切：

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### 准备训练

​		在实际编写训练循环之前，我们需要定义几个对象。第一个是我们将用于迭代批处理的数据加载器。但是在定义这些数据加载器之前，我们需要对tokenized_datasets进行一些后处理，以处理一些Trainer自动为我们完成的工作。具体来说，我们需要：

+ 删除与模型不期望的值对应的列(比如sentence1和sentence2列)。
+ 将列标签重命名为标签(因为模型希望参数被命名为标签)。
+ 设置数据集的格式，使它们返回PyTorch张量而不是列表。

​		我们的tokenized_datasets对于每个步骤都有一个方法：

```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

​		然后，我们可以检查结果是否只有模型能够接受的列:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

​		现在，我们可以轻松地定义数据加载器了:

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

​		为了快速检查数据处理中是否有错误，我们可以这样检查一批:

```python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

运行结果：

```python
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

​		请注意，实际的形状可能会略有不同，因为我们为训练数据加载器设置了shuffle=True，我们填充到批内的最大长度。

​		现在我们已经完全完成了数据预处理(对于任何ML从业者来说，这是一个令人满意但难以实现的目标)，让我们转向模型。我们完全像在前一节中那样实例化它:

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

​		为了确保训练期间一切顺利，我们将我们的批传递给这个模型:

```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

运行结果：

```python
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

​		当提供标签时，所有transformer模型都将返回损失，我们还将得到logits(批处理中每个输入都是2，因此一个张量大小为8 x 2)。

​		我们几乎已经准备好编写我们的训练循环了!我们只是遗漏了两件事:优化器和学习率调度器。因为我们试图手工复制Trainer所做的事情，所以我们将使用相同的默认值。Trainer使用的优化器是AdamW，与Adam相同，但对权重衰减正则化进行了调整(参见Ilya Loshchilov和Frank Hutter的解耦权重衰减正则化):

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

​		最后，默认使用的学习率调度器只是从最大值(5e-5)到0的线性衰减。为了正确地定义它，我们需要知道将要进行的训练步骤的数量，这是我们想要运行的epoch的数量乘以训练批次的数量(这是我们训练数据加载器的长度)。默认情况下，Trainer使用三个epoch，所以我们将按照它来操作:

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

运行结果：

```python
1377
```



### 循环训练

​		最后一件事:如果我们可以使用GPU，我们会想要使用GPU(在CPU上，训练可能需要几个小时而不是几分钟)。为此，我们定义了一个设备，用于放置我们的模型和批次:

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

运行结果：

```python
device(type='cuda')
```

​		我们现在准备训练了!为了了解训练何时结束，我们使用tqdm库在训练步骤数量上添加一个进度条:

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

​		你可以看到训练循环的核心看起来很像介绍中的那个。我们没有要求任何报告，所以这个训练循环不会告诉我们任何关于模型的情况。我们需要为此添加一个评估循环。



### 评价循环

​		和前面一样，我们将使用Datasets库提供的指标。我们已经看到了metric.compute()方法，但是当我们使用方法add_batch()进行预测循环时，metrics实际上可以为我们积累批次。一旦我们积累了所有批处理，我们就可以使用metric.compute()获得最终结果。下面是如何在求值循环中实现所有这些:

```python
from datasets import load_metric

metric = load_metric("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

运行结果：

```python
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

​		同样，由于模型头部初始化和数据洗牌的随机性，结果会略有不同，但它们应该在同一个范围内。



### 用加速增压你的训练循环

​		我们之前定义的训练循环在单个CPU或GPU上运行良好。但是使用Accelerate库，只需稍加调整，我们就可以在多个gpu或tpu上进行分布式训练。从创建训练和验证数据加载器开始，我们的手动训练循环是这样的：

```python
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

以下是改变:

```python
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

​		要添加的第一行是导入行。第二行实例化一个Accelerator对象，该对象将查看环境并初始化适当的分布式设置。Accelerate为你处理设备放置，所以你可以删除将模型放在设备上的行(或者，如果你喜欢，将它们更改为使用accelerator.device而不是device)。

​		然后，主要的工作在发送数据加载器、模型和优化器到accelerator.prepare()的行中完成。这将把这些对象包装在适当的容器中，以确保您的分布式培训按预期工作。剩下要做的更改是删除将批处理放到设备上的那一行(同样，如果你想要保留它，你只需将其更改为使用accelerator.device)，并将loss.backward()替换为accelerator.backward(loss)。

​		如果您想复制并粘贴它来播放，下面是使用Accelerate完成训练循环的样子:

```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

​		将其放入train.py脚本中，将使该脚本可以在任何类型的分布式设置上运行。要在您的分布式安装中尝试，请运行该命令:

```python
accelerate config
```

​		它将提示您回答几个问题，并将答案转储到该命令使用的配置文件中:

```python
accelerate launch train.py
```

​		它将启动分布式训练。

​		如果您想在Notebook中尝试这一点(例如，在Colab上使用tpu测试它)，只需将代码粘贴在training_function()中，并使用运行最后一个单元格:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```



## 微调，检查

​		这很有趣！在前两章中，您了解了模型和分词器，现在您知道了如何针对自己的数据对其进行微调。回顾一下，在本章中，您将：

+ 了解中心中的数据集
+ 学习了如何加载和预处理数据集，包括使用动态填充和排序规则器
+ 实现您自己的模型微调和评估
+ 实施了较低级别的训练循环
+ 使用“加速”轻松调整训练循环，使其适用于多个 GPU 或 TPU







